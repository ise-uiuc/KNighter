### Report Summary

File:| /scratch/chenyuan-data/linux-debug/arch/x86/kernel/cpu/mce/core.c
---|---
Warning:| line 659, column 12
Multiplication occurs in a narrower type and is widened after; possible
overflow before assignment/addition to wide type

### Annotated Source Code


585   | static int uc_decode_notifier(struct notifier_block *nb, unsigned long val,
586   |  void *data)
587   | {
588   |  struct mce *mce = (struct mce *)data;
589   |  unsigned long pfn;
590   |
591   |  if (!mce || !mce_usable_address(mce))
592   |  return NOTIFY_DONE;
593   |
594   |  if (mce->severity != MCE_AO_SEVERITY &&
595   | 	    mce->severity != MCE_DEFERRED_SEVERITY)
596   |  return NOTIFY_DONE;
597   |
598   | 	pfn = (mce->addr & MCI_ADDR_PHYSADDR) >> PAGE_SHIFT;
599   |  if (!memory_failure(pfn, 0)) {
600   | 		set_mce_nospec(pfn);
601   | 		mce->kflags |= MCE_HANDLED_UC;
602   | 	}
603   |
604   |  return NOTIFY_OK;
605   | }
606   |
607   | static struct notifier_block mce_uc_nb = {
608   | 	.notifier_call	= uc_decode_notifier,
609   | 	.priority	= MCE_PRIO_UC,
610   | };
611   |
612   | static int mce_default_notifier(struct notifier_block *nb, unsigned long val,
613   |  void *data)
614   | {
615   |  struct mce *m = (struct mce *)data;
616   |
617   |  if (!m)
618   |  return NOTIFY_DONE;
619   |
620   |  if (mca_cfg.print_all || !m->kflags)
621   | 		__print_mce(m);
622   |
623   |  return NOTIFY_DONE;
624   | }
625   |
626   | static struct notifier_block mce_default_nb = {
627   | 	.notifier_call	= mce_default_notifier,
628   |  /* lowest prio, we want it to run last. */
629   | 	.priority	= MCE_PRIO_LOWEST,
630   | };
631   |
632   | /*
633   |  * Read ADDR and MISC registers.
634   |  */
635   | static noinstr void mce_read_aux(struct mce *m, int i)
636   | {
637   |  if (m->status & MCI_STATUS_MISCV)
    15←Assuming the condition is false→
    16←Taking false branch→
638   | 		m->misc = mce_rdmsrl(mca_msr_reg(i, MCA_MISC));
639   |
640   |  if (m->status & MCI_STATUS_ADDRV) {
    17←Assuming the condition is false→
    18←Taking false branch→
641   | 		m->addr = mce_rdmsrl(mca_msr_reg(i, MCA_ADDR));
642   |
643   |  /*
644   |  * Mask the reported address by the reported granularity.
645   |  */
646   |  if (mca_cfg.ser && (m->status & MCI_STATUS_MISCV)) {
647   | 			u8 shift = MCI_MISC_ADDR_LSB(m->misc);
648   | 			m->addr >>= shift;
649   | 			m->addr <<= shift;
650   | 		}
651   |
652   | 		smca_extract_err_addr(m);
653   | 	}
654   |
655   |  if (mce_flags.smca) {
    19←Assuming field 'smca' is not equal to 0→
    20←Taking true branch→
656   |  m->ipid = mce_rdmsrl(MSR_AMD64_SMCA_MCx_IPID(i));
657   |
658   |  if (m->status & MCI_STATUS_SYNDV)
    21←Assuming the condition is true→
    22←Taking true branch→
659   |  m->synd = mce_rdmsrl(MSR_AMD64_SMCA_MCx_SYND(i));
    23←Multiplication occurs in a narrower type and is widened after; possible overflow before assignment/addition to wide type
660   | 	}
661   | }
662   |
663   | DEFINE_PER_CPU(unsigned, mce_poll_count);
664   |
665   | /*
666   |  * Poll for corrected events or events that happened before reset.
667   |  * Those are just logged through /dev/mcelog.
668   |  *
669   |  * This is executed in standard interrupt context.
670   |  *
671   |  * Note: spec recommends to panic for fatal unsignalled
672   |  * errors here. However this would be quite problematic --
673   |  * we would need to reimplement the Monarch handling and
674   |  * it would mess up the exclusion between exception handler
675   |  * and poll handler -- * so we skip this for now.
676   |  * These cases should not happen anyways, or only when the CPU
677   |  * is already totally * confused. In this case it's likely it will
678   |  * not fully execute the machine check handler either.
679   |  */
680   | bool machine_check_poll(enum mcp_flags flags, mce_banks_t *b)
681   | {
682   |  struct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);
683   | 	bool error_seen = false;
684   |  struct mce m;
685   |  int i;
686   |
687   |  this_cpu_inc(mce_poll_count);
688   |
689   | 	mce_gather_info(&m, NULL);
851   |  return false;
852   |
853   | 	mc1_status = mce_rdmsrl(MSR_IA32_MCx_STATUS(1));
854   |
855   |  /* Check for a software-recoverable data fetch error. */
856   |  if ((mc1_status &
857   | 	     (MCI_STATUS_VAL | MCI_STATUS_OVER | MCI_STATUS_UC | MCI_STATUS_EN |
858   |  MCI_STATUS_ADDRV | MCI_STATUS_MISCV | MCI_STATUS_PCC |
859   |  MCI_STATUS_AR | MCI_STATUS_S)) ==
860   | 	     (MCI_STATUS_VAL |                   MCI_STATUS_UC | MCI_STATUS_EN |
861   |  MCI_STATUS_ADDRV | MCI_STATUS_MISCV |
862   |  MCI_STATUS_AR | MCI_STATUS_S)) {
863   | 		misc_enable &= ~MSR_IA32_MISC_ENABLE_FAST_STRING;
864   | 		mce_wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);
865   | 		mce_wrmsrl(MSR_IA32_MCx_STATUS(1), 0);
866   |
867   |  instrumentation_begin();
868   |  pr_err_once("Erratum detected, disable fast string copy instructions.\n");
869   |  instrumentation_end();
870   |
871   |  return true;
872   | 	}
873   |
874   |  return false;
875   | }
876   |
877   | /*
878   |  * Some Zen-based Instruction Fetch Units set EIPV=RIPV=0 on poison consumption
879   |  * errors. This means mce_gather_info() will not save the "ip" and "cs" registers.
880   |  *
881   |  * However, the context is still valid, so save the "cs" register for later use.
882   |  *
883   |  * The "ip" register is truly unknown, so don't save it or fixup EIPV/RIPV.
884   |  *
885   |  * The Instruction Fetch Unit is at MCA bank 1 for all affected systems.
886   |  */
887   | static __always_inline void quirk_zen_ifu(int bank, struct mce *m, struct pt_regs *regs)
888   | {
889   |  if (bank != 1)
890   |  return;
891   |  if (!(m->status & MCI_STATUS_POISON))
892   |  return;
893   |
894   | 	m->cs = regs->cs;
895   | }
896   |
897   | /*
898   |  * Do a quick check if any of the events requires a panic.
899   |  * This decides if we keep the events around or clear them.
900   |  */
901   | static __always_inline int mce_no_way_out(struct mce *m, char **msg, unsigned long *validp,
902   |  struct pt_regs *regs)
903   | {
904   |  char *tmp = *msg;
905   |  int i;
906   |
907   |  for (i = 0; i < this_cpu_read(mce_num_banks); i++) {
    1Loop condition is false.  Exiting loop→
    2←Control jumps to 'case 4:'  at line 907→
    3← Execution continues on line 907→
    4←Assuming the condition is true→
    5←Loop condition is true.  Entering loop body→
908   |  m->status = mce_rdmsrl(mca_msr_reg(i, MCA_STATUS));
909   |  if (!(m->status & MCI_STATUS_VAL))
    6←Assuming the condition is false→
    7←Taking false branch→
910   |  continue;
911   |
912   |  arch___set_bit(i, validp);
913   |  if (mce_flags.snb_ifu_quirk)
    8←Assuming field 'snb_ifu_quirk' is 0→
    9←Taking false branch→
914   | 			quirk_sandybridge_ifu(i, m, regs);
915   |
916   |  if (mce_flags.zen_ifu_quirk)
    10←Assuming field 'zen_ifu_quirk' is 0→
    11←Taking false branch→
917   | 			quirk_zen_ifu(i, m, regs);
918   |
919   |  m->bank = i;
920   |  if (mce_severity(m, regs, &tmp, true) >= MCE_PANIC_SEVERITY) {
    12←Assuming the condition is true→
    13←Taking true branch→
921   |  mce_read_aux(m, i);
    14←Calling 'mce_read_aux'→
922   | 			*msg = tmp;
923   |  return 1;
924   | 		}
925   | 	}
926   |  return 0;
927   | }
928   |
929   | /*
930   |  * Variable to establish order between CPUs while scanning.
931   |  * Each CPU spins initially until executing is equal its number.
932   |  */
933   | static atomic_t mce_executing;
934   |
935   | /*
936   |  * Defines order of CPUs on entry. First CPU becomes Monarch.
937   |  */
938   | static atomic_t mce_callin;
939   |
940   | /*
941   |  * Track which CPUs entered the MCA broadcast synchronization and which not in
942   |  * order to print holdouts.
943   |  */
944   | static cpumask_t mce_missing_cpus = CPU_MASK_ALL;
945   |
946   | /*
947   |  * Check if a timeout waiting for other CPUs happened.
948   |  */
949   | static noinstr int mce_timed_out(u64 *t, const char *msg)
950   | {
951   |  int ret = 0;
