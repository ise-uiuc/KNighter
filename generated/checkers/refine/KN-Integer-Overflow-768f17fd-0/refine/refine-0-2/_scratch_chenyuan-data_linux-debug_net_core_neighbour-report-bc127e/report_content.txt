### Report Summary

File:| /scratch/chenyuan-data/linux-debug/net/core/neighbour.c
---|---
Warning:| line 533, column 2
Shift done in 32-bit, widened after; cast left operand to 64-bit before <<

### Annotated Source Code


62    | #endif
63    |
64    | /*
65    |  Neighbour hash table buckets are protected with rwlock tbl->lock.
66    |
67    |  - All the scans/updates to hash buckets MUST be made under this lock.
68    |  - NOTHING clever should be made under this lock: no callbacks
69    |  to protocol backends, no attempts to send something to network.
70    |  It will result in deadlocks, if backend/driver wants to use neighbour
71    |  cache.
72    |  - If the entry requires some non-trivial actions, increase
73    |  its reference count and release table lock.
74    |
75    |  Neighbour entries are protected:
76    |  - with reference count.
77    |  - with rwlock neigh->lock
78    |
79    |  Reference count prevents destruction.
80    |
81    |  neigh->lock mainly serializes ll address data and its validity state.
82    |  However, the same lock is used to protect another entry fields:
83    |  - timer
84    |  - resolution queue
85    |
86    |  Again, nothing clever shall be made under neigh->lock,
87    |  the most complicated procedure, which we allow is dev->hard_header.
88    |  It is supposed, that dev->hard_header is simplistic and does
89    |  not make callbacks to neighbour tables.
90    |  */
91    |
92    | static int neigh_blackhole(struct neighbour *neigh, struct sk_buff *skb)
93    | {
94    | 	kfree_skb(skb);
95    |  return -ENETDOWN;
96    | }
97    |
98    | static void neigh_cleanup_and_release(struct neighbour *neigh)
99    | {
100   | 	trace_neigh_cleanup_and_release(neigh, 0);
101   | 	__neigh_notify(neigh, RTM_DELNEIGH, 0, 0);
102   | 	call_netevent_notifiers(NETEVENT_NEIGH_UPDATE, neigh);
103   | 	neigh_release(neigh);
104   | }
105   |
106   | /*
107   |  * It is random distribution in the interval (1/2)*base...(3/2)*base.
108   |  * It corresponds to default IPv6 settings and is not overridable,
109   |  * because it is really reasonable choice.
110   |  */
111   |
112   | unsigned long neigh_rand_reach_time(unsigned long base)
113   | {
114   |  return base ? get_random_u32_below(base) + (base >> 1) : 0;
115   | }
116   | EXPORT_SYMBOL(neigh_rand_reach_time);
117   |
118   | static void neigh_mark_dead(struct neighbour *n)
119   | {
120   | 	n->dead = 1;
121   |  if (!list_empty(&n->gc_list)) {
122   | 		list_del_init(&n->gc_list);
123   | 		atomic_dec(&n->tbl->gc_entries);
124   | 	}
125   |  if (!list_empty(&n->managed_list))
126   | 		list_del_init(&n->managed_list);
127   | }
128   |
129   | static void neigh_update_gc_list(struct neighbour *n)
130   | {
131   | 	bool on_gc_list, exempt_from_gc;
132   |
133   |  write_lock_bh(&n->tbl->lock);
134   |  write_lock(&n->lock);
135   |  if (n->dead)
136   |  goto out;
137   |
138   |  /* remove from the gc list if new state is permanent or if neighbor
139   |  * is externally learned; otherwise entry should be on the gc list
140   |  */
141   | 	exempt_from_gc = n->nud_state & NUD_PERMANENT ||
142   | 			 n->flags & NTF_EXT_LEARNED;
143   | 	on_gc_list = !list_empty(&n->gc_list);
144   |
481   | 	gc_thresh3 = READ_ONCE(tbl->gc_thresh3);
482   |  if (entries >= gc_thresh3 ||
483   | 	    (entries >= READ_ONCE(tbl->gc_thresh2) &&
484   |  time_after(now, READ_ONCE(tbl->last_flush) + 5 * HZ))) {
485   |  if (!neigh_forced_gc(tbl) && entries >= gc_thresh3) {
486   |  net_info_ratelimited("%s: neighbor table overflow!\n",
487   |  tbl->id);
488   |  NEIGH_CACHE_STAT_INC(tbl, table_fulls);
489   |  goto out_entries;
490   | 		}
491   | 	}
492   |
493   | do_alloc:
494   | 	n = kzalloc(tbl->entry_size + dev->neigh_priv_len, GFP_ATOMIC);
495   |  if (!n)
496   |  goto out_entries;
497   |
498   | 	__skb_queue_head_init(&n->arp_queue);
499   |  rwlock_init(&n->lock);
500   |  seqlock_init(&n->ha_lock);
501   | 	n->updated	  = n->used = now;
502   | 	n->nud_state	  = NUD_NONE;
503   | 	n->output	  = neigh_blackhole;
504   | 	n->flags	  = flags;
505   |  seqlock_init(&n->hh.hh_lock);
506   | 	n->parms	  = neigh_parms_clone(&tbl->parms);
507   |  timer_setup(&n->timer, neigh_timer_handler, 0);
508   |
509   |  NEIGH_CACHE_STAT_INC(tbl, allocs);
510   | 	n->tbl		  = tbl;
511   | 	refcount_set(&n->refcnt, 1);
512   | 	n->dead		  = 1;
513   | 	INIT_LIST_HEAD(&n->gc_list);
514   | 	INIT_LIST_HEAD(&n->managed_list);
515   |
516   | 	atomic_inc(&tbl->entries);
517   | out:
518   |  return n;
519   |
520   | out_entries:
521   |  if (!exempt_from_gc)
522   | 		atomic_dec(&tbl->gc_entries);
523   |  goto out;
524   | }
525   |
526   | static void neigh_get_hash_rnd(u32 *x)
527   | {
528   | 	*x = get_random_u32() | 1;
529   | }
530   |
531   | static struct neigh_hash_table *neigh_hash_alloc(unsigned int shift)
532   | {
533   |  size_t size = (1 << shift) * sizeof(struct neighbour *);
    8←Shift done in 32-bit, widened after; cast left operand to 64-bit before <<
534   |  struct neigh_hash_table *ret;
535   |  struct neighbour __rcu **buckets;
536   |  int i;
537   |
538   | 	ret = kmalloc(sizeof(*ret), GFP_ATOMIC);
539   |  if (!ret)
540   |  return NULL;
541   |  if (size <= PAGE_SIZE) {
542   | 		buckets = kzalloc(size, GFP_ATOMIC);
543   | 	} else {
544   | 		buckets = (struct neighbour __rcu **)
545   | 			  __get_free_pages(GFP_ATOMIC | __GFP_ZERO,
546   | 					   get_order(size));
547   | 		kmemleak_alloc(buckets, size, 1, GFP_ATOMIC);
548   | 	}
549   |  if (!buckets) {
550   | 		kfree(ret);
551   |  return NULL;
552   | 	}
553   | 	ret->hash_buckets = buckets;
554   | 	ret->hash_shift = shift;
555   |  for (i = 0; i < NEIGH_NUM_HASH_RND; i++)
556   | 		neigh_get_hash_rnd(&ret->hash_rnd[i]);
557   |  return ret;
558   | }
559   |
560   | static void neigh_hash_free_rcu(struct rcu_head *head)
561   | {
562   |  struct neigh_hash_table *nht = container_of(head,
563   |  struct neigh_hash_table,
1724  | 		p->dev = dev;
1725  | 		write_pnet(&p->net, net);
1726  | 		p->sysctl_table = NULL;
1727  |
1728  |  if (ops->ndo_neigh_setup && ops->ndo_neigh_setup(dev, p)) {
1729  | 			netdev_put(dev, &p->dev_tracker);
1730  | 			kfree(p);
1731  |  return NULL;
1732  | 		}
1733  |
1734  |  write_lock_bh(&tbl->lock);
1735  | 		list_add(&p->list, &tbl->parms.list);
1736  |  write_unlock_bh(&tbl->lock);
1737  |
1738  | 		neigh_parms_data_state_cleanall(p);
1739  | 	}
1740  |  return p;
1741  | }
1742  | EXPORT_SYMBOL(neigh_parms_alloc);
1743  |
1744  | static void neigh_rcu_free_parms(struct rcu_head *head)
1745  | {
1746  |  struct neigh_parms *parms =
1747  |  container_of(head, struct neigh_parms, rcu_head);
1748  |
1749  | 	neigh_parms_put(parms);
1750  | }
1751  |
1752  | void neigh_parms_release(struct neigh_table *tbl, struct neigh_parms *parms)
1753  | {
1754  |  if (!parms || parms == &tbl->parms)
1755  |  return;
1756  |  write_lock_bh(&tbl->lock);
1757  | 	list_del(&parms->list);
1758  | 	parms->dead = 1;
1759  |  write_unlock_bh(&tbl->lock);
1760  | 	netdev_put(parms->dev, &parms->dev_tracker);
1761  | 	call_rcu(&parms->rcu_head, neigh_rcu_free_parms);
1762  | }
1763  | EXPORT_SYMBOL(neigh_parms_release);
1764  |
1765  | static void neigh_parms_destroy(struct neigh_parms *parms)
1766  | {
1767  | 	kfree(parms);
1768  | }
1769  |
1770  | static struct lock_class_key neigh_table_proxy_queue_class;
1771  |
1772  | static struct neigh_table *neigh_tables[NEIGH_NR_TABLES] __read_mostly;
1773  |
1774  | void neigh_table_init(int index, struct neigh_table *tbl)
1775  | {
1776  |  unsigned long now = jiffies;
1777  |  unsigned long phsize;
1778  |
1779  | 	INIT_LIST_HEAD(&tbl->parms_list);
1780  | 	INIT_LIST_HEAD(&tbl->gc_list);
1781  | 	INIT_LIST_HEAD(&tbl->managed_list);
1782  |
1783  | 	list_add(&tbl->parms.list, &tbl->parms_list);
1784  | 	write_pnet(&tbl->parms.net, &init_net);
1785  | 	refcount_set(&tbl->parms.refcnt, 1);
1786  | 	tbl->parms.reachable_time =
1787  | 			  neigh_rand_reach_time(NEIGH_VAR(&tbl->parms, BASE_REACHABLE_TIME));
1788  | 	tbl->parms.qlen = 0;
1789  |
1790  | 	tbl->stats = alloc_percpu(struct neigh_statistics);
1791  |  if (!tbl->stats)
    1Assuming field 'stats' is non-null→
    2←Taking false branch→
1792  | 		panic("cannot create neighbour cache statistics");
1793  |
1794  | #ifdef CONFIG_PROC_FS
1795  |  if (!proc_create_seq_data(tbl->id, 0, init_net.proc_net_stat,
    3←Assuming the condition is false→
1796  |  &neigh_stat_seq_ops, tbl))
1797  | 		panic("cannot create neighbour proc dir entry");
1798  | #endif
1799  |
1800  |  RCU_INIT_POINTER(tbl->nht, neigh_hash_alloc(3));
    4←Taking false branch→
    5←Taking false branch→
    6←Loop condition is false.  Exiting loop→
    7←Calling 'neigh_hash_alloc'→
1801  |
1802  | 	phsize = (PNEIGH_HASHMASK + 1) * sizeof(struct pneigh_entry *);
1803  | 	tbl->phash_buckets = kzalloc(phsize, GFP_KERNEL);
1804  |
1805  |  if (!tbl->nht || !tbl->phash_buckets)
1806  | 		panic("cannot allocate neighbour cache hashes");
1807  |
1808  |  if (!tbl->entry_size)
1809  | 		tbl->entry_size = ALIGN(offsetof(struct neighbour, primary_key) +
1810  |  tbl->key_len, NEIGH_PRIV_ALIGN);
1811  |  else
1812  |  WARN_ON(tbl->entry_size % NEIGH_PRIV_ALIGN);
1813  |
1814  |  rwlock_init(&tbl->lock);
1815  |
1816  |  INIT_DEFERRABLE_WORK(&tbl->gc_work, neigh_periodic_work);
1817  | 	queue_delayed_work(system_power_efficient_wq, &tbl->gc_work,
1818  | 			tbl->parms.reachable_time);
1819  |  INIT_DEFERRABLE_WORK(&tbl->managed_work, neigh_managed_work);
1820  | 	queue_delayed_work(system_power_efficient_wq, &tbl->managed_work, 0);
1821  |
1822  |  timer_setup(&tbl->proxy_timer, neigh_proxy_process, 0);
1823  | 	skb_queue_head_init_class(&tbl->proxy_queue,
1824  | 			&neigh_table_proxy_queue_class);
1825  |
1826  | 	tbl->last_flush = now;
1827  | 	tbl->last_rand	= now + tbl->parms.reachable_time * 20;
1828  |
1829  | 	neigh_tables[index] = tbl;
1830  | }
