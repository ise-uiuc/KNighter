### Report Summary

File:| /scratch/chenyuan-data/linux-debug/drivers/pci/controller/cadence/pcie-
cadence-ep.c
---|---
Warning:| line 455, column 38
Off-by-one bound check: use '>= MAX' instead of '> MAX' for index validation

### Annotated Source Code


1     | // SPDX-License-Identifier: GPL-2.0
2     | // Copyright (c) 2017 Cadence
3     | // Cadence PCIe endpoint controller driver.
4     | // Author: Cyrille Pitchen <cyrille.pitchen@free-electrons.com>
5     |
6     | #include <linux/bitfield.h>
7     | #include <linux/delay.h>
8     | #include <linux/kernel.h>
9     | #include <linux/of.h>
10    | #include <linux/pci-epc.h>
11    | #include <linux/platform_device.h>
12    | #include <linux/sizes.h>
13    |
14    | #include "pcie-cadence.h"
15    |
16    | #define CDNS_PCIE_EP_MIN_APERTURE		128	/* 128 bytes */
17    | #define CDNS_PCIE_EP_IRQ_PCI_ADDR_NONE		0x1
18    | #define CDNS_PCIE_EP_IRQ_PCI_ADDR_LEGACY	0x3
19    |
20    | static u8 cdns_pcie_get_fn_from_vfn(struct cdns_pcie *pcie, u8 fn, u8 vfn)
21    | {
22    | 	u32 cap = CDNS_PCIE_EP_FUNC_SRIOV_CAP_OFFSET;
23    | 	u32 first_vf_offset, stride;
24    |
25    |  if (vfn == 0)
26    |  return fn;
27    |
28    | 	first_vf_offset = cdns_pcie_ep_fn_readw(pcie, fn, cap + PCI_SRIOV_VF_OFFSET);
29    | 	stride = cdns_pcie_ep_fn_readw(pcie, fn, cap +  PCI_SRIOV_VF_STRIDE);
30    | 	fn = fn + first_vf_offset + ((vfn - 1) * stride);
31    |
32    |  return fn;
33    | }
34    |
35    | static int cdns_pcie_ep_write_header(struct pci_epc *epc, u8 fn, u8 vfn,
36    |  struct pci_epf_header *hdr)
37    | {
38    |  struct cdns_pcie_ep *ep = epc_get_drvdata(epc);
39    | 	u32 cap = CDNS_PCIE_EP_FUNC_SRIOV_CAP_OFFSET;
40    |  struct cdns_pcie *pcie = &ep->pcie;
41    | 	u32 reg;
42    |
43    |  if (vfn > 1) {
44    |  dev_err(&epc->dev, "Only Virtual Function #1 has deviceID\n");
45    |  return -EINVAL;
46    | 	} else if (vfn == 1) {
47    | 		reg = cap + PCI_SRIOV_VF_DID;
48    | 		cdns_pcie_ep_fn_writew(pcie, fn, reg, hdr->deviceid);
49    |  return 0;
50    | 	}
51    |
52    | 	cdns_pcie_ep_fn_writew(pcie, fn, PCI_DEVICE_ID, hdr->deviceid);
53    | 	cdns_pcie_ep_fn_writeb(pcie, fn, PCI_REVISION_ID, hdr->revid);
54    | 	cdns_pcie_ep_fn_writeb(pcie, fn, PCI_CLASS_PROG, hdr->progif_code);
55    | 	cdns_pcie_ep_fn_writew(pcie, fn, PCI_CLASS_DEVICE,
56    | 			       hdr->subclass_code | hdr->baseclass_code << 8);
381   | static int cdns_pcie_ep_send_msi_irq(struct cdns_pcie_ep *ep, u8 fn, u8 vfn,
382   | 				     u8 interrupt_num)
383   | {
384   |  struct cdns_pcie *pcie = &ep->pcie;
385   | 	u32 cap = CDNS_PCIE_EP_FUNC_MSI_CAP_OFFSET;
386   | 	u16 flags, mme, data, data_mask;
387   | 	u8 msi_count;
388   | 	u64 pci_addr, pci_addr_mask = 0xff;
389   |
390   | 	fn = cdns_pcie_get_fn_from_vfn(pcie, fn, vfn);
391   |
392   |  /* Check whether the MSI feature has been enabled by the PCI host. */
393   | 	flags = cdns_pcie_ep_fn_readw(pcie, fn, cap + PCI_MSI_FLAGS);
394   |  if (!(flags & PCI_MSI_FLAGS_ENABLE))
395   |  return -EINVAL;
396   |
397   |  /* Get the number of enabled MSIs */
398   | 	mme = FIELD_GET(PCI_MSI_FLAGS_QSIZE, flags);
399   | 	msi_count = 1 << mme;
400   |  if (!interrupt_num || interrupt_num > msi_count)
401   |  return -EINVAL;
402   |
403   |  /* Compute the data value to be written. */
404   | 	data_mask = msi_count - 1;
405   | 	data = cdns_pcie_ep_fn_readw(pcie, fn, cap + PCI_MSI_DATA_64);
406   | 	data = (data & ~data_mask) | ((interrupt_num - 1) & data_mask);
407   |
408   |  /* Get the PCI address where to write the data into. */
409   | 	pci_addr = cdns_pcie_ep_fn_readl(pcie, fn, cap + PCI_MSI_ADDRESS_HI);
410   | 	pci_addr <<= 32;
411   | 	pci_addr |= cdns_pcie_ep_fn_readl(pcie, fn, cap + PCI_MSI_ADDRESS_LO);
412   | 	pci_addr &= GENMASK_ULL(63, 2);
413   |
414   |  /* Set the outbound region if needed. */
415   |  if (unlikely(ep->irq_pci_addr != (pci_addr & ~pci_addr_mask) ||
416   |  ep->irq_pci_fn != fn)) {
417   |  /* First region was reserved for IRQ writes. */
418   | 		cdns_pcie_set_outbound_region(pcie, 0, fn, 0,
419   | 					      false,
420   | 					      ep->irq_phys_addr,
421   | 					      pci_addr & ~pci_addr_mask,
422   | 					      pci_addr_mask + 1);
423   | 		ep->irq_pci_addr = (pci_addr & ~pci_addr_mask);
424   | 		ep->irq_pci_fn = fn;
425   | 	}
426   |  writel(data, ep->irq_cpu_addr + (pci_addr & pci_addr_mask));
427   |
428   |  return 0;
429   | }
430   |
431   | static int cdns_pcie_ep_map_msi_irq(struct pci_epc *epc, u8 fn, u8 vfn,
432   | 				    phys_addr_t addr, u8 interrupt_num,
433   | 				    u32 entry_size, u32 *msi_data,
434   | 				    u32 *msi_addr_offset)
435   | {
436   |  struct cdns_pcie_ep *ep = epc_get_drvdata(epc);
437   | 	u32 cap = CDNS_PCIE_EP_FUNC_MSI_CAP_OFFSET;
438   |  struct cdns_pcie *pcie = &ep->pcie;
439   | 	u64 pci_addr, pci_addr_mask = 0xff;
440   | 	u16 flags, mme, data, data_mask;
441   | 	u8 msi_count;
442   |  int ret;
443   |  int i;
444   |
445   | 	fn = cdns_pcie_get_fn_from_vfn(pcie, fn, vfn);
446   |
447   |  /* Check whether the MSI feature has been enabled by the PCI host. */
448   | 	flags = cdns_pcie_ep_fn_readw(pcie, fn, cap + PCI_MSI_FLAGS);
449   |  if (!(flags & PCI_MSI_FLAGS_ENABLE))
    1Assuming the condition is false→
    2←Taking false branch→
450   |  return -EINVAL;
451   |
452   |  /* Get the number of enabled MSIs */
453   |  mme = FIELD_GET(PCI_MSI_FLAGS_QSIZE, flags);
    3←Taking false branch→
    4←Loop condition is false.  Exiting loop→
    5←Taking false branch→
    6←Loop condition is false.  Exiting loop→
    7←'?' condition is true→
    8←Assuming right operand of bit shift is non-negative but less than 32→
    9←Taking false branch→
    10←Loop condition is false.  Exiting loop→
    11←Taking false branch→
    12←Loop condition is false.  Exiting loop→
    13←Assuming right operand of bit shift is non-negative but less than 64→
    14←Assuming right operand of bit shift is non-negative but less than 64→
    15←Assuming the condition is false→
    16←Taking false branch→
    17←Loop condition is false.  Exiting loop→
    18←Assuming right operand of bit shift is non-negative but less than 32→
454   |  msi_count = 1 << mme;
    19←Assuming right operand of bit shift is non-negative but less than 32→
455   |  if (!interrupt_num || interrupt_num > msi_count)
    20←Assuming 'interrupt_num' is not equal to 0→
    21←Assuming 'interrupt_num' is <= 'msi_count'→
    22←Off-by-one bound check: use '>= MAX' instead of '> MAX' for index validation
456   |  return -EINVAL;
457   |
458   |  /* Compute the data value to be written. */
459   | 	data_mask = msi_count - 1;
460   | 	data = cdns_pcie_ep_fn_readw(pcie, fn, cap + PCI_MSI_DATA_64);
461   | 	data = data & ~data_mask;
462   |
463   |  /* Get the PCI address where to write the data into. */
464   | 	pci_addr = cdns_pcie_ep_fn_readl(pcie, fn, cap + PCI_MSI_ADDRESS_HI);
465   | 	pci_addr <<= 32;
466   | 	pci_addr |= cdns_pcie_ep_fn_readl(pcie, fn, cap + PCI_MSI_ADDRESS_LO);
467   | 	pci_addr &= GENMASK_ULL(63, 2);
468   |
469   |  for (i = 0; i < interrupt_num; i++) {
470   | 		ret = cdns_pcie_ep_map_addr(epc, fn, vfn, addr,
471   | 					    pci_addr & ~pci_addr_mask,
472   | 					    entry_size);
473   |  if (ret)
474   |  return ret;
475   | 		addr = addr + entry_size;
476   | 	}
477   |
478   | 	*msi_data = data;
479   | 	*msi_addr_offset = pci_addr & pci_addr_mask;
480   |
481   |  return 0;
482   | }
483   |
484   | static int cdns_pcie_ep_send_msix_irq(struct cdns_pcie_ep *ep, u8 fn, u8 vfn,
485   | 				      u16 interrupt_num)
