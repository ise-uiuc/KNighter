### Report Summary

File:| net/sched/cls_u32.c
---|---
Warning:| line 1392, column 8
Copying partially initialized struct with padding to user; zero-initialize
before export

### Annotated Source Code


1279  |  struct tc_u_knode *n;
1280  |  unsigned int h;
1281  |  int err;
1282  |
1283  |  for (ht = rtnl_dereference(tp_c->hlist);
1284  | 	     ht;
1285  | 	     ht = rtnl_dereference(ht->next)) {
1286  |  if (ht->prio != tp->prio)
1287  |  continue;
1288  |
1289  |  /* When adding filters to a new dev, try to offload the
1290  |  * hashtable first. When removing, do the filters before the
1291  |  * hashtable.
1292  |  */
1293  |  if (add && !tc_skip_hw(ht->flags)) {
1294  | 			err = u32_reoffload_hnode(tp, ht, add, cb, cb_priv,
1295  | 						  extack);
1296  |  if (err)
1297  |  return err;
1298  | 		}
1299  |
1300  |  for (h = 0; h <= ht->divisor; h++) {
1301  |  for (n = rtnl_dereference(ht->ht[h]);
1302  | 			     n;
1303  | 			     n = rtnl_dereference(n->next)) {
1304  |  if (tc_skip_hw(n->flags))
1305  |  continue;
1306  |
1307  | 				err = u32_reoffload_knode(tp, n, add, cb,
1308  | 							  cb_priv, extack);
1309  |  if (err)
1310  |  return err;
1311  | 			}
1312  | 		}
1313  |
1314  |  if (!add && !tc_skip_hw(ht->flags))
1315  | 			u32_reoffload_hnode(tp, ht, add, cb, cb_priv, extack);
1316  | 	}
1317  |
1318  |  return 0;
1319  | }
1320  |
1321  | static void u32_bind_class(void *fh, u32 classid, unsigned long cl, void *q,
1322  |  unsigned long base)
1323  | {
1324  |  struct tc_u_knode *n = fh;
1325  |
1326  | 	tc_cls_bind_class(classid, cl, q, &n->res, base);
1327  | }
1328  |
1329  | static int u32_dump(struct net *net, struct tcf_proto *tp, void *fh,
1330  |  struct sk_buff *skb, struct tcmsg *t, bool rtnl_held)
1331  | {
1332  |  struct tc_u_knode *n = fh;
1333  |  struct tc_u_hnode *ht_up, *ht_down;
1334  |  struct nlattr *nest;
1335  |
1336  |  if (n == NULL)
    1Assuming 'n' is not equal to NULL→
    2←Taking false branch→
1337  |  return skb->len;
1338  |
1339  |  t->tcm_handle = n->handle;
1340  |
1341  | 	nest = nla_nest_start_noflag(skb, TCA_OPTIONS);
1342  |  if (nest == NULL)
    3←Assuming 'nest' is not equal to NULL→
    4←Taking false branch→
1343  |  goto nla_put_failure;
1344  |
1345  |  if (TC_U32_KEY(n->handle) == 0) {
    5←Assuming the condition is false→
    6←Taking false branch→
1346  |  struct tc_u_hnode *ht = fh;
1347  | 		u32 divisor = ht->divisor + 1;
1348  |
1349  |  if (nla_put_u32(skb, TCA_U32_DIVISOR, divisor))
1350  |  goto nla_put_failure;
1351  | 	} else {
1352  | #ifdef CONFIG_CLS_U32_PERF
1353  |  struct tc_u32_pcnt *gpf;
1354  |  int cpu;
1355  | #endif
1356  |
1357  |  if (nla_put(skb, TCA_U32_SEL, struct_size(&n->sel, keys, n->sel.nkeys),
    7←Assuming the condition is false→
    8←Taking false branch→
1358  |  &n->sel))
1359  |  goto nla_put_failure;
1360  |
1361  |  ht_up = rtnl_dereference(n->ht_up);
    9←Assuming the condition is false→
    10←Loop condition is false.  Exiting loop→
1362  |  if (ht_up) {
    11←Assuming 'ht_up' is null→
1363  | 			u32 htid = n->handle & 0xFFFFF000;
1364  |  if (nla_put_u32(skb, TCA_U32_HASH, htid))
1365  |  goto nla_put_failure;
1366  | 		}
1367  |  if (n->res.classid &&
    12←Assuming field 'classid' is 0→
1368  | 		    nla_put_u32(skb, TCA_U32_CLASSID, n->res.classid))
1369  |  goto nla_put_failure;
1370  |
1371  |  ht_down = rtnl_dereference(n->ht_down);
    13←Assuming the condition is true→
    14←Assuming the condition is false→
    15←Loop condition is false.  Exiting loop→
1372  |  if (ht_down &&
    16←Assuming 'ht_down' is null→
1373  | 		    nla_put_u32(skb, TCA_U32_LINK, ht_down->handle))
1374  |  goto nla_put_failure;
1375  |
1376  |  if (n->flags && nla_put_u32(skb, TCA_U32_FLAGS, n->flags))
    17←Assuming field 'flags' is 0→
1377  |  goto nla_put_failure;
1378  |
1379  | #ifdef CONFIG_CLS_U32_MARK
1380  |  if ((n->val || n->mask)) {
    18←Assuming field 'val' is not equal to 0→
1381  |  struct tc_u32_mark mark = {.val = n->val,
1382  | 						   .mask = n->mask,
1383  | 						   .success = 0};
1384  |  int cpum;
1385  |
1386  |  for_each_possible_cpu(cpum) {
    19←Assuming 'cpum' is >= 'nr_cpu_ids'→
    20←Loop condition is false. Execution continues on line 1392→
1387  | 				__u32 cnt = *per_cpu_ptr(n->pcpu_success, cpum);
1388  |
1389  | 				mark.success += cnt;
1390  | 			}
1391  |
1392  |  if (nla_put(skb, TCA_U32_MARK, sizeof(mark), &mark))
    21←Copying partially initialized struct with padding to user; zero-initialize before export
1393  |  goto nla_put_failure;
1394  | 		}
1395  | #endif
1396  |
1397  |  if (tcf_exts_dump(skb, &n->exts) < 0)
1398  |  goto nla_put_failure;
1399  |
1400  |  if (n->ifindex) {
1401  |  struct net_device *dev;
1402  | 			dev = __dev_get_by_index(net, n->ifindex);
1403  |  if (dev && nla_put_string(skb, TCA_U32_INDEV, dev->name))
1404  |  goto nla_put_failure;
1405  | 		}
1406  | #ifdef CONFIG_CLS_U32_PERF
1407  | 		gpf = kzalloc(struct_size(gpf, kcnts, n->sel.nkeys), GFP_KERNEL);
1408  |  if (!gpf)
1409  |  goto nla_put_failure;
1410  |
1411  |  for_each_possible_cpu(cpu) {
1412  |  int i;
1413  |  struct tc_u32_pcnt *pf = per_cpu_ptr(n->pf, cpu);
1414  |
1415  | 			gpf->rcnt += pf->rcnt;
1416  | 			gpf->rhit += pf->rhit;
1417  |  for (i = 0; i < n->sel.nkeys; i++)
1418  | 				gpf->kcnts[i] += pf->kcnts[i];
1419  | 		}
1420  |
1421  |  if (nla_put_64bit(skb, TCA_U32_PCNT, struct_size(gpf, kcnts, n->sel.nkeys),
1422  | 				  gpf, TCA_U32_PAD)) {
