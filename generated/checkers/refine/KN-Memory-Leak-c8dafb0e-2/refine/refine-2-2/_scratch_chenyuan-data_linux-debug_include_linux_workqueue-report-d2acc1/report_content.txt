### Report Summary

File:| /scratch/chenyuan-data/linux-debug/./include/linux/workqueue.h
---|---
Warning:| line 734, column 2
Missing hwrm_req_drop() after successful hwrm_req_init()

### Annotated Source Code


302   | #define DB_CP_IRQ_DIS_FLAGS	(DB_KEY_CP | DB_IRQ_DIS)
303   |
304   | #define BNXT_CP_DB_IRQ_DIS(db)						\
305   |  writel(DB_CP_IRQ_DIS_FLAGS, db)
306   |
307   | #define BNXT_DB_CQ(db, idx)						\
308   |  writel(DB_CP_FLAGS | DB_RING_IDX(db, idx), (db)->doorbell)
309   |
310   | #define BNXT_DB_NQ_P5(db, idx)						\
311   |  bnxt_writeq(bp, (db)->db_key64 | DBR_TYPE_NQ | DB_RING_IDX(db, idx),\
312   |  (db)->doorbell)
313   |
314   | #define BNXT_DB_NQ_P7(db, idx)						\
315   |  bnxt_writeq(bp, (db)->db_key64 | DBR_TYPE_NQ_MASK |		\
316   |  DB_RING_IDX(db, idx), (db)->doorbell)
317   |
318   | #define BNXT_DB_CQ_ARM(db, idx)						\
319   |  writel(DB_CP_REARM_FLAGS | DB_RING_IDX(db, idx), (db)->doorbell)
320   |
321   | #define BNXT_DB_NQ_ARM_P5(db, idx)					\
322   |  bnxt_writeq(bp, (db)->db_key64 | DBR_TYPE_NQ_ARM |		\
323   |  DB_RING_IDX(db, idx), (db)->doorbell)
324   |
325   | static void bnxt_db_nq(struct bnxt *bp, struct bnxt_db_info *db, u32 idx)
326   | {
327   |  if (bp->flags & BNXT_FLAG_CHIP_P7)
328   |  BNXT_DB_NQ_P7(db, idx);
329   |  else if (bp->flags & BNXT_FLAG_CHIP_P5_PLUS)
330   |  BNXT_DB_NQ_P5(db, idx);
331   |  else
332   |  BNXT_DB_CQ(db, idx);
333   | }
334   |
335   | static void bnxt_db_nq_arm(struct bnxt *bp, struct bnxt_db_info *db, u32 idx)
336   | {
337   |  if (bp->flags & BNXT_FLAG_CHIP_P5_PLUS)
338   |  BNXT_DB_NQ_ARM_P5(db, idx);
339   |  else
340   |  BNXT_DB_CQ_ARM(db, idx);
341   | }
342   |
343   | static void bnxt_db_cq(struct bnxt *bp, struct bnxt_db_info *db, u32 idx)
344   | {
345   |  if (bp->flags & BNXT_FLAG_CHIP_P5_PLUS)
346   | 		bnxt_writeq(bp, db->db_key64 | DBR_TYPE_CQ_ARMALL |
347   |  DB_RING_IDX(db, idx), db->doorbell);
348   |  else
349   |  BNXT_DB_CQ(db, idx);
350   | }
351   |
352   | static void bnxt_queue_fw_reset_work(struct bnxt *bp, unsigned long delay)
353   | {
354   |  if (!(test_bit(BNXT_STATE_IN_FW_RESET, &bp->state)))
    10←Assuming the condition is false→
    11←Taking false branch→
355   |  return;
356   |
357   |  if (BNXT_PF(bp))
    12←Assuming the condition is false→
    13←Taking false branch→
358   | 		queue_delayed_work(bnxt_pf_wq, &bp->fw_reset_task, delay);
359   |  else
360   |  schedule_delayed_work(&bp->fw_reset_task, delay);
    14←Calling 'schedule_delayed_work'→
361   | }
362   |
363   | static void __bnxt_queue_sp_work(struct bnxt *bp)
364   | {
365   |  if (BNXT_PF(bp))
366   | 		queue_work(bnxt_pf_wq, &bp->sp_task);
367   |  else
368   | 		schedule_work(&bp->sp_task);
369   | }
370   |
371   | static void bnxt_queue_sp_work(struct bnxt *bp, unsigned int event)
372   | {
373   | 	set_bit(event, &bp->sp_event);
374   | 	__bnxt_queue_sp_work(bp);
375   | }
376   |
377   | static void bnxt_sched_reset_rxr(struct bnxt *bp, struct bnxt_rx_ring_info *rxr)
378   | {
379   |  if (!rxr->bnapi->in_reset) {
380   | 		rxr->bnapi->in_reset = true;
381   |  if (bp->flags & BNXT_FLAG_CHIP_P5_PLUS)
382   | 			set_bit(BNXT_RESET_TASK_SP_EVENT, &bp->sp_event);
383   |  else
384   | 			set_bit(BNXT_RST_RING_SP_EVENT, &bp->sp_event);
385   | 		__bnxt_queue_sp_work(bp);
386   | 	}
387   | 	rxr->rx_next_cons = 0xffff;
388   | }
389   |
390   | void bnxt_sched_reset_txr(struct bnxt *bp, struct bnxt_tx_ring_info *txr,
9242  | {
9243  |  struct hwrm_queue_qportcfg_output *resp;
9244  |  struct hwrm_queue_qportcfg_input *req;
9245  | 	u8 i, j, *qptr;
9246  | 	bool no_rdma;
9247  |  int rc = 0;
9248  |
9249  | 	rc = hwrm_req_init(bp, req, HWRM_QUEUE_QPORTCFG);
9250  |  if (rc)
9251  |  return rc;
9252  |
9253  | 	resp = hwrm_req_hold(bp, req);
9254  | 	rc = hwrm_req_send(bp, req);
9255  |  if (rc)
9256  |  goto qportcfg_exit;
9257  |
9258  |  if (!resp->max_configurable_queues) {
9259  | 		rc = -EINVAL;
9260  |  goto qportcfg_exit;
9261  | 	}
9262  | 	bp->max_tc = resp->max_configurable_queues;
9263  | 	bp->max_lltc = resp->max_configurable_lossless_queues;
9264  |  if (bp->max_tc > BNXT_MAX_QUEUE)
9265  | 		bp->max_tc = BNXT_MAX_QUEUE;
9266  |
9267  | 	no_rdma = !(bp->flags & BNXT_FLAG_ROCE_CAP);
9268  | 	qptr = &resp->queue_id0;
9269  |  for (i = 0, j = 0; i < bp->max_tc; i++) {
9270  | 		bp->q_info[j].queue_id = *qptr;
9271  | 		bp->q_ids[i] = *qptr++;
9272  | 		bp->q_info[j].queue_profile = *qptr++;
9273  | 		bp->tc_to_qidx[j] = j;
9274  |  if (!BNXT_CNPQ(bp->q_info[j].queue_profile) ||
9275  | 		    (no_rdma && BNXT_PF(bp)))
9276  | 			j++;
9277  | 	}
9278  | 	bp->max_q = bp->max_tc;
9279  | 	bp->max_tc = max_t(u8, j, 1);
9280  |
9281  |  if (resp->queue_cfg_info & QUEUE_QPORTCFG_RESP_QUEUE_CFG_INFO_ASYM_CFG)
9282  | 		bp->max_tc = 1;
9283  |
9284  |  if (bp->max_lltc > bp->max_tc)
9285  | 		bp->max_lltc = bp->max_tc;
9286  |
9287  | qportcfg_exit:
9288  | 	hwrm_req_drop(bp, req);
9289  |  return rc;
9290  | }
9291  |
9292  | static int bnxt_hwrm_poll(struct bnxt *bp)
9293  | {
9294  |  struct hwrm_ver_get_input *req;
9295  |  int rc;
9296  |
9297  | 	rc = hwrm_req_init(bp, req, HWRM_VER_GET);
9298  |  if (rc)
9299  |  return rc;
9300  |
9301  | 	req->hwrm_intf_maj = HWRM_VERSION_MAJOR;
9302  | 	req->hwrm_intf_min = HWRM_VERSION_MINOR;
9303  | 	req->hwrm_intf_upd = HWRM_VERSION_UPDATE;
9304  |
9305  | 	hwrm_req_flags(bp, req, BNXT_HWRM_CTX_SILENT | BNXT_HWRM_FULL_WAIT);
9306  | 	rc = hwrm_req_send(bp, req);
9307  |  return rc;
9308  | }
9309  |
9310  | static int bnxt_hwrm_ver_get(struct bnxt *bp)
9311  | {
9312  |  struct hwrm_ver_get_output *resp;
9313  |  struct hwrm_ver_get_input *req;
9314  | 	u16 fw_maj, fw_min, fw_bld, fw_rsv;
9315  | 	u32 dev_caps_cfg, hwrm_ver;
9316  |  int rc, len;
9317  |
9318  | 	rc = hwrm_req_init(bp, req, HWRM_VER_GET);
9319  |  if (rc)
9320  |  return rc;
9321  |
9322  | 	hwrm_req_flags(bp, req, BNXT_HWRM_FULL_WAIT);
9323  | 	bp->hwrm_max_req_len = HWRM_MAX_REQ_LEN;
9324  | 	req->hwrm_intf_maj = HWRM_VERSION_MAJOR;
9325  | 	req->hwrm_intf_min = HWRM_VERSION_MINOR;
9326  | 	req->hwrm_intf_upd = HWRM_VERSION_UPDATE;
9327  |
9328  | 	resp = hwrm_req_hold(bp, req);
9329  | 	rc = hwrm_req_send(bp, req);
9330  |  if (rc)
9331  |  goto hwrm_ver_get_exit;
9332  |
9333  |  memcpy(&bp->ver_resp, resp, sizeof(struct hwrm_ver_get_output));
9334  |
9335  | 	bp->hwrm_spec_code = resp->hwrm_intf_maj_8b << 16 |
9336  | 			     resp->hwrm_intf_min_8b << 8 |
9337  | 			     resp->hwrm_intf_upd_8b;
13705 | }
13706 |
13707 | static void bnxt_reset_all(struct bnxt *bp)
13708 | {
13709 |  struct bnxt_fw_health *fw_health = bp->fw_health;
13710 |  int i, rc;
13711 |
13712 |  if (bp->fw_cap & BNXT_FW_CAP_ERR_RECOVER_RELOAD) {
13713 | 		bnxt_fw_reset_via_optee(bp);
13714 | 		bp->fw_reset_timestamp = jiffies;
13715 |  return;
13716 | 	}
13717 |
13718 |  if (fw_health->flags & ERROR_RECOVERY_QCFG_RESP_FLAGS_HOST) {
13719 |  for (i = 0; i < fw_health->fw_reset_seq_cnt; i++)
13720 | 			bnxt_fw_reset_writel(bp, i);
13721 | 	} else if (fw_health->flags & ERROR_RECOVERY_QCFG_RESP_FLAGS_CO_CPU) {
13722 |  struct hwrm_fw_reset_input *req;
13723 |
13724 | 		rc = hwrm_req_init(bp, req, HWRM_FW_RESET);
13725 |  if (!rc) {
13726 | 			req->target_id = cpu_to_le16(HWRM_TARGET_ID_KONG);
13727 | 			req->embedded_proc_type = FW_RESET_REQ_EMBEDDED_PROC_TYPE_CHIP;
13728 | 			req->selfrst_status = FW_RESET_REQ_SELFRST_STATUS_SELFRSTASAP;
13729 | 			req->flags = FW_RESET_REQ_FLAGS_RESET_GRACEFUL;
13730 | 			rc = hwrm_req_send(bp, req);
13731 | 		}
13732 |  if (rc != -ENODEV)
13733 | 			netdev_warn(bp->dev, "Unable to reset FW rc=%d\n", rc);
13734 | 	}
13735 | 	bp->fw_reset_timestamp = jiffies;
13736 | }
13737 |
13738 | static bool bnxt_fw_reset_timeout(struct bnxt *bp)
13739 | {
13740 |  return time_after(jiffies, bp->fw_reset_timestamp +
13741 |  (bp->fw_reset_max_dsecs * HZ / 10));
13742 | }
13743 |
13744 | static void bnxt_fw_reset_abort(struct bnxt *bp, int rc)
13745 | {
13746 | 	clear_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
13747 |  if (bp->fw_reset_state != BNXT_FW_RESET_STATE_POLL_VF) {
13748 | 		bnxt_ulp_start(bp, rc);
13749 | 		bnxt_dl_health_fw_status_update(bp, false);
13750 | 	}
13751 | 	bp->fw_reset_state = 0;
13752 | 	dev_close(bp->dev);
13753 | }
13754 |
13755 | static void bnxt_fw_reset_task(struct work_struct *work)
13756 | {
13757 |  struct bnxt *bp = container_of(work, struct bnxt, fw_reset_task.work);
13758 |  int rc = 0;
13759 |
13760 |  if (!test_bit(BNXT_STATE_IN_FW_RESET, &bp->state)) {
    1Assuming the condition is true→
    2←Assuming the condition is false→
    3←Taking false branch→
13761 | 		netdev_err(bp->dev, "bnxt_fw_reset_task() called when not in fw reset mode!\n");
13762 |  return;
13763 | 	}
13764 |
13765 |  switch (bp->fw_reset_state) {
    4←Control jumps to 'case 4:'  at line 13860→
13766 |  case BNXT_FW_RESET_STATE_POLL_VF: {
13767 |  int n = bnxt_get_registered_vfs(bp);
13768 |  int tmo;
13769 |
13770 |  if (n < 0) {
13771 | 			netdev_err(bp->dev, "Firmware reset aborted, subsequent func_qcfg cmd failed, rc = %d, %d msecs since reset timestamp\n",
13772 | 				   n, jiffies_to_msecs(jiffies -
13773 | 				   bp->fw_reset_timestamp));
13774 |  goto fw_reset_abort;
13775 | 		} else if (n > 0) {
13776 |  if (bnxt_fw_reset_timeout(bp)) {
13777 | 				clear_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
13778 | 				bp->fw_reset_state = 0;
13779 | 				netdev_err(bp->dev, "Firmware reset aborted, bnxt_get_registered_vfs() returns %d\n",
13780 | 					   n);
13781 |  return;
13782 | 			}
13783 | 			bnxt_queue_fw_reset_work(bp, HZ / 10);
13784 |  return;
13785 | 		}
13786 | 		bp->fw_reset_timestamp = jiffies;
13787 | 		rtnl_lock();
13788 |  if (test_bit(BNXT_STATE_ABORT_ERR, &bp->state)) {
13789 | 			bnxt_fw_reset_abort(bp, rc);
13790 | 			rtnl_unlock();
13791 |  return;
13792 | 		}
13793 | 		bnxt_fw_reset_close(bp);
13794 |  if (bp->fw_cap & BNXT_FW_CAP_ERR_RECOVER_RELOAD) {
13795 | 			bp->fw_reset_state = BNXT_FW_RESET_STATE_POLL_FW_DOWN;
13810 | 		    !bnxt_fw_reset_timeout(bp)) {
13811 | 			bnxt_queue_fw_reset_work(bp, HZ / 5);
13812 |  return;
13813 | 		}
13814 |
13815 |  if (!bp->fw_health->primary) {
13816 | 			u32 wait_dsecs = bp->fw_health->normal_func_wait_dsecs;
13817 |
13818 | 			bp->fw_reset_state = BNXT_FW_RESET_STATE_ENABLE_DEV;
13819 | 			bnxt_queue_fw_reset_work(bp, wait_dsecs * HZ / 10);
13820 |  return;
13821 | 		}
13822 | 		bp->fw_reset_state = BNXT_FW_RESET_STATE_RESET_FW;
13823 | 	}
13824 |  fallthrough;
13825 |  case BNXT_FW_RESET_STATE_RESET_FW:
13826 | 		bnxt_reset_all(bp);
13827 | 		bp->fw_reset_state = BNXT_FW_RESET_STATE_ENABLE_DEV;
13828 | 		bnxt_queue_fw_reset_work(bp, bp->fw_reset_min_dsecs * HZ / 10);
13829 |  return;
13830 |  case BNXT_FW_RESET_STATE_ENABLE_DEV:
13831 | 		bnxt_inv_fw_health_reg(bp);
13832 |  if (test_bit(BNXT_STATE_FW_FATAL_COND, &bp->state) &&
13833 | 		    !bp->fw_reset_min_dsecs) {
13834 | 			u16 val;
13835 |
13836 | 			pci_read_config_word(bp->pdev, PCI_SUBSYSTEM_ID, &val);
13837 |  if (val == 0xffff) {
13838 |  if (bnxt_fw_reset_timeout(bp)) {
13839 | 					netdev_err(bp->dev, "Firmware reset aborted, PCI config space invalid\n");
13840 | 					rc = -ETIMEDOUT;
13841 |  goto fw_reset_abort;
13842 | 				}
13843 | 				bnxt_queue_fw_reset_work(bp, HZ / 1000);
13844 |  return;
13845 | 			}
13846 | 		}
13847 | 		clear_bit(BNXT_STATE_FW_FATAL_COND, &bp->state);
13848 | 		clear_bit(BNXT_STATE_FW_NON_FATAL_COND, &bp->state);
13849 |  if (test_and_clear_bit(BNXT_STATE_FW_ACTIVATE_RESET, &bp->state) &&
13850 | 		    !test_bit(BNXT_STATE_FW_ACTIVATE, &bp->state))
13851 | 			bnxt_dl_remote_reload(bp);
13852 |  if (pci_enable_device(bp->pdev)) {
13853 | 			netdev_err(bp->dev, "Cannot re-enable PCI device\n");
13854 | 			rc = -ENODEV;
13855 |  goto fw_reset_abort;
13856 | 		}
13857 | 		pci_set_master(bp->pdev);
13858 | 		bp->fw_reset_state = BNXT_FW_RESET_STATE_POLL_FW;
13859 |  fallthrough;
13860 |  case BNXT_FW_RESET_STATE_POLL_FW:
13861 |  bp->hwrm_cmd_timeout = SHORT_HWRM_CMD_TIMEOUT;
13862 |  rc = bnxt_hwrm_poll(bp);
13863 |  if (rc) {
    5←Assuming 'rc' is 0→
    6←Taking false branch→
13864 |  if (bnxt_fw_reset_timeout(bp)) {
13865 | 				netdev_err(bp->dev, "Firmware reset aborted\n");
13866 |  goto fw_reset_abort_status;
13867 | 			}
13868 | 			bnxt_queue_fw_reset_work(bp, HZ / 5);
13869 |  return;
13870 | 		}
13871 |  bp->hwrm_cmd_timeout = DFLT_HWRM_CMD_TIMEOUT;
13872 | 		bp->fw_reset_state = BNXT_FW_RESET_STATE_OPENING;
13873 |  fallthrough;
13874 |  case BNXT_FW_RESET_STATE_OPENING:
13875 |  while (!rtnl_trylock()) {
    7←Assuming the condition is true→
    8←Loop condition is true.  Entering loop body→
13876 |  bnxt_queue_fw_reset_work(bp, HZ / 10);
    9←Calling 'bnxt_queue_fw_reset_work'→
13877 |  return;
13878 | 		}
13879 | 		rc = bnxt_open(bp->dev);
13880 |  if (rc) {
13881 | 			netdev_err(bp->dev, "bnxt_open() failed during FW reset\n");
13882 | 			bnxt_fw_reset_abort(bp, rc);
13883 | 			rtnl_unlock();
13884 |  return;
13885 | 		}
13886 |
13887 |  if ((bp->fw_cap & BNXT_FW_CAP_ERROR_RECOVERY) &&
13888 | 		    bp->fw_health->enabled) {
13889 | 			bp->fw_health->last_fw_reset_cnt =
13890 | 				bnxt_fw_health_readl(bp, BNXT_FW_RESET_CNT_REG);
13891 | 		}
13892 | 		bp->fw_reset_state = 0;
13893 |  /* Make sure fw_reset_state is 0 before clearing the flag */
13894 |  smp_mb__before_atomic();
13895 | 		clear_bit(BNXT_STATE_IN_FW_RESET, &bp->state);
13896 | 		bnxt_ulp_start(bp, 0);
13897 | 		bnxt_reenable_sriov(bp);
13898 | 		bnxt_vf_reps_alloc(bp);
13899 | 		bnxt_vf_reps_open(bp);
13900 | 		bnxt_ptp_reapply_pps(bp);
13901 | 		clear_bit(BNXT_STATE_FW_ACTIVATE, &bp->state);
13902 |  if (test_and_clear_bit(BNXT_STATE_RECOVER, &bp->state)) {
13903 | 			bnxt_dl_health_fw_recovery_done(bp);
13904 | 			bnxt_dl_health_fw_status_update(bp, true);
13905 | 		}
13906 | 		rtnl_unlock();
566   | extern void workqueue_set_min_active(struct workqueue_struct *wq,
567   |  int min_active);
568   | extern struct work_struct *current_work(void);
569   | extern bool current_is_workqueue_rescuer(void);
570   | extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
571   | extern unsigned int work_busy(struct work_struct *work);
572   | extern __printf(1, 2) void set_worker_desc(const char *fmt, ...);
573   | extern void print_worker_info(const char *log_lvl, struct task_struct *task);
574   | extern void show_all_workqueues(void);
575   | extern void show_freezable_workqueues(void);
576   | extern void show_one_workqueue(struct workqueue_struct *wq);
577   | extern void wq_worker_comm(char *buf, size_t size, struct task_struct *task);
578   |
579   | /**
580   |  * queue_work - queue work on a workqueue
581   |  * @wq: workqueue to use
582   |  * @work: work to queue
583   |  *
584   |  * Returns %false if @work was already on a queue, %true otherwise.
585   |  *
586   |  * We queue the work to the CPU on which it was submitted, but if the CPU dies
587   |  * it can be processed by another CPU.
588   |  *
589   |  * Memory-ordering properties:  If it returns %true, guarantees that all stores
590   |  * preceding the call to queue_work() in the program order will be visible from
591   |  * the CPU which will execute @work by the time such work executes, e.g.,
592   |  *
593   |  * { x is initially 0 }
594   |  *
595   |  *   CPU0				CPU1
596   |  *
597   |  *   WRITE_ONCE(x, 1);			[ @work is being executed ]
598   |  *   r0 = queue_work(wq, work);		  r1 = READ_ONCE(x);
599   |  *
600   |  * Forbids: r0 == true && r1 == 0
601   |  */
602   | static inline bool queue_work(struct workqueue_struct *wq,
603   |  struct work_struct *work)
604   | {
605   |  return queue_work_on(WORK_CPU_UNBOUND, wq, work);
606   | }
607   |
608   | /**
609   |  * queue_delayed_work - queue work on a workqueue after delay
610   |  * @wq: workqueue to use
611   |  * @dwork: delayable work to queue
612   |  * @delay: number of jiffies to wait before queueing
613   |  *
614   |  * Equivalent to queue_delayed_work_on() but tries to use the local CPU.
615   |  */
616   | static inline bool queue_delayed_work(struct workqueue_struct *wq,
617   |  struct delayed_work *dwork,
618   |  unsigned long delay)
619   | {
620   |  return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
621   | }
622   |
623   | /**
624   |  * mod_delayed_work - modify delay of or queue a delayed work
625   |  * @wq: workqueue to use
626   |  * @dwork: work to queue
627   |  * @delay: number of jiffies to wait before queueing
628   |  *
629   |  * mod_delayed_work_on() on local CPU.
630   |  */
631   | static inline bool mod_delayed_work(struct workqueue_struct *wq,
632   |  struct delayed_work *dwork,
633   |  unsigned long delay)
634   | {
635   |  return mod_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
636   | }
637   |
638   | /**
639   |  * schedule_work_on - put work task on a specific cpu
640   |  * @cpu: cpu to put the work task on
641   |  * @work: job to be done
642   |  *
643   |  * This puts a job on a specific cpu
644   |  */
645   | static inline bool schedule_work_on(int cpu, struct work_struct *work)
646   | {
647   |  return queue_work_on(cpu, system_wq, work);
648   | }
649   |
650   | /**
681   | ({									\
682   |  __warn_flushing_systemwide_wq();				\
683   |  __flush_workqueue(system_wq);					\
684   | })
685   |
686   | #define flush_workqueue(wq)						\
687   | ({									\
688   |  struct workqueue_struct *_wq = (wq);				\
689   |  \
690   |  if ((__builtin_constant_p(_wq == system_wq) &&			\
691   |  _wq == system_wq) ||					\
692   |  (__builtin_constant_p(_wq == system_highpri_wq) &&		\
693   |  _wq == system_highpri_wq) ||				\
694   |  (__builtin_constant_p(_wq == system_long_wq) &&		\
695   |  _wq == system_long_wq) ||					\
696   |  (__builtin_constant_p(_wq == system_unbound_wq) &&		\
697   |  _wq == system_unbound_wq) ||				\
698   |  (__builtin_constant_p(_wq == system_freezable_wq) &&	\
699   |  _wq == system_freezable_wq) ||				\
700   |  (__builtin_constant_p(_wq == system_power_efficient_wq) &&	\
701   |  _wq == system_power_efficient_wq) ||			\
702   |  (__builtin_constant_p(_wq == system_freezable_power_efficient_wq) && \
703   |  _wq == system_freezable_power_efficient_wq))		\
704   |  __warn_flushing_systemwide_wq();			\
705   |  __flush_workqueue(_wq);						\
706   | })
707   |
708   | /**
709   |  * schedule_delayed_work_on - queue work in global workqueue on CPU after delay
710   |  * @cpu: cpu to use
711   |  * @dwork: job to be done
712   |  * @delay: number of jiffies to wait
713   |  *
714   |  * After waiting for a given time this puts a job in the kernel-global
715   |  * workqueue on the specified CPU.
716   |  */
717   | static inline bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork,
718   |  unsigned long delay)
719   | {
720   |  return queue_delayed_work_on(cpu, system_wq, dwork, delay);
721   | }
722   |
723   | /**
724   |  * schedule_delayed_work - put work task in global workqueue after delay
725   |  * @dwork: job to be done
726   |  * @delay: number of jiffies to wait or 0 for immediate execution
727   |  *
728   |  * After waiting for a given time this puts a job in the kernel-global
729   |  * workqueue.
730   |  */
731   | static inline bool schedule_delayed_work(struct delayed_work *dwork,
732   |  unsigned long delay)
733   | {
734   |  return queue_delayed_work(system_wq, dwork, delay);
    15←Missing hwrm_req_drop() after successful hwrm_req_init()
735   | }
736   |
737   | #ifndef CONFIG_SMP
738   | static inline long work_on_cpu(int cpu, long (*fn)(void *), void *arg)
739   | {
740   |  return fn(arg);
741   | }
742   | static inline long work_on_cpu_safe(int cpu, long (*fn)(void *), void *arg)
743   | {
744   |  return fn(arg);
745   | }
746   | #else
747   | long work_on_cpu_key(int cpu, long (*fn)(void *),
748   |  void *arg, struct lock_class_key *key);
749   | /*
750   |  * A new key is defined for each caller to make sure the work
751   |  * associated with the function doesn't share its locking class.
752   |  */
753   | #define work_on_cpu(_cpu, _fn, _arg)			\
754   | ({							\
755   |  static struct lock_class_key __key;		\
756   |  \
757   |  work_on_cpu_key(_cpu, _fn, _arg, &__key);	\
758   | })
759   |
760   | long work_on_cpu_safe_key(int cpu, long (*fn)(void *),
761   |  void *arg, struct lock_class_key *key);
762   |
763   | /*
764   |  * A new key is defined for each caller to make sure the work
