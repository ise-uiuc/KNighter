### Report Summary

File:| /scratch/chenyuan-data/linux-debug/drivers/pci/msi/msi.c
---|---
Warning:| line 571, column 39
32-bit multiply widens to 64-bit after overflow; cast an operand to 64-bit
before the multiply

### Annotated Source Code


31    |  struct pci_bus *bus;
32    |
33    |  /* MSI must be globally enabled and supported by the device */
34    |  if (!pci_msi_enable)
35    |  return 0;
36    |
37    |  if (!dev || dev->no_msi)
38    |  return 0;
39    |
40    |  /*
41    |  * You can't ask to have 0 or less MSIs configured.
42    |  *  a) it's stupid ..
43    |  *  b) the list manipulation code assumes nvec >= 1.
44    |  */
45    |  if (nvec < 1)
46    |  return 0;
47    |
48    |  /*
49    |  * Any bridge which does NOT route MSI transactions from its
50    |  * secondary bus to its primary bus must set NO_MSI flag on
51    |  * the secondary pci_bus.
52    |  *
53    |  * The NO_MSI flag can either be set directly by:
54    |  * - arch-specific PCI host bus controller drivers (deprecated)
55    |  * - quirks for specific PCI bridges
56    |  *
57    |  * or indirectly by platform-specific PCI host bridge drivers by
58    |  * advertising the 'msi_domain' property, which results in
59    |  * the NO_MSI flag when no MSI domain is found for this bridge
60    |  * at probe time.
61    |  */
62    |  for (bus = dev->bus; bus; bus = bus->parent)
63    |  if (bus->bus_flags & PCI_BUS_FLAGS_NO_MSI)
64    |  return 0;
65    |
66    |  return 1;
67    | }
68    |
69    | static void pcim_msi_release(void *pcidev)
70    | {
71    |  struct pci_dev *dev = pcidev;
72    |
73    | 	dev->is_msi_managed = false;
74    | 	pci_free_irq_vectors(dev);
75    | }
76    |
77    | /*
78    |  * Needs to be separate from pcim_release to prevent an ordering problem
79    |  * vs. msi_device_data_release() in the MSI core code.
80    |  */
81    | static int pcim_setup_msi_release(struct pci_dev *dev)
82    | {
83    |  int ret;
84    |
85    |  if (!pci_is_managed(dev) || dev->is_msi_managed)
86    |  return 0;
87    |
88    | 	ret = devm_add_action(&dev->dev, pcim_msi_release, dev);
89    |  if (!ret)
90    | 		dev->is_msi_managed = true;
91    |  return ret;
92    | }
93    |
94    | /*
95    |  * Ordering vs. devres: msi device data has to be installed first so that
96    |  * pcim_msi_release() is invoked before it on device release.
97    |  */
98    | static int pci_setup_msi_context(struct pci_dev *dev)
99    | {
100   |  int ret = msi_setup_device_data(&dev->dev);
101   |
102   |  if (!ret)
103   | 		ret = pcim_setup_msi_release(dev);
104   |  return ret;
105   | }
106   |
107   | /*
108   |  * Helper functions for mask/unmask and MSI message handling
109   |  */
110   |
111   | void pci_msi_update_mask(struct msi_desc *desc, u32 clear, u32 set)
112   | {
113   | 	raw_spinlock_t *lock = &to_pci_dev(desc->dev)->msi_lock;
114   |  unsigned long flags;
115   |
116   |  if (!desc->pci.msi_attrib.can_mask)
117   |  return;
118   |
119   |  raw_spin_lock_irqsave(lock, flags);
120   | 	desc->pci.msi_mask &= ~clear;
121   | 	desc->pci.msi_mask |= set;
122   | 	pci_write_config_dword(msi_desc_to_pci_dev(desc), desc->pci.mask_pos,
123   | 			       desc->pci.msi_mask);
124   |  raw_spin_unlock_irqrestore(lock, flags);
125   | }
126   |
127   | /**
128   |  * pci_msi_mask_irq - Generic IRQ chip callback to mask PCI/MSI interrupts
129   |  * @data:	pointer to irqdata associated to that interrupt
130   |  */
131   | void pci_msi_mask_irq(struct irq_data *data)
132   | {
133   |  struct msi_desc *desc = irq_data_get_msi_desc(data);
134   |
493   | {
494   |  return true;
495   | }
496   |
497   | void __pci_restore_msi_state(struct pci_dev *dev)
498   | {
499   |  struct msi_desc *entry;
500   | 	u16 control;
501   |
502   |  if (!dev->msi_enabled)
503   |  return;
504   |
505   | 	entry = irq_get_msi_desc(dev->irq);
506   |
507   | 	pci_intx_for_msi(dev, 0);
508   | 	pci_msi_set_enable(dev, 0);
509   |  if (arch_restore_msi_irqs(dev))
510   | 		__pci_write_msi_msg(entry, &entry->msg);
511   |
512   | 	pci_read_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, &control);
513   | 	pci_msi_update_mask(entry, 0, 0);
514   | 	control &= ~PCI_MSI_FLAGS_QSIZE;
515   | 	control |= PCI_MSI_FLAGS_ENABLE |
516   |  FIELD_PREP(PCI_MSI_FLAGS_QSIZE, entry->pci.msi_attrib.multiple);
517   | 	pci_write_config_word(dev, dev->msi_cap + PCI_MSI_FLAGS, control);
518   | }
519   |
520   | void pci_msi_shutdown(struct pci_dev *dev)
521   | {
522   |  struct msi_desc *desc;
523   |
524   |  if (!pci_msi_enable || !dev || !dev->msi_enabled)
525   |  return;
526   |
527   | 	pci_msi_set_enable(dev, 0);
528   | 	pci_intx_for_msi(dev, 1);
529   | 	dev->msi_enabled = 0;
530   |
531   |  /* Return the device with MSI unmasked as initial states */
532   | 	desc = msi_first_desc(&dev->dev, MSI_DESC_ALL);
533   |  if (!WARN_ON_ONCE(!desc))
534   | 		pci_msi_unmask(desc, msi_multi_mask(desc));
535   |
536   |  /* Restore dev->irq to its default pin-assertion IRQ */
537   | 	dev->irq = desc->pci.msi_attrib.default_irq;
538   | 	pcibios_alloc_irq(dev);
539   | }
540   |
541   | /* PCI/MSI-X specific functionality */
542   |
543   | static void pci_msix_clear_and_set_ctrl(struct pci_dev *dev, u16 clear, u16 set)
544   | {
545   | 	u16 ctrl;
546   |
547   | 	pci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &ctrl);
548   | 	ctrl &= ~clear;
549   | 	ctrl |= set;
550   | 	pci_write_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, ctrl);
551   | }
552   |
553   | static void __iomem *msix_map_region(struct pci_dev *dev,
554   |  unsigned int nr_entries)
555   | {
556   |  resource_size_t phys_addr;
557   | 	u32 table_offset;
558   |  unsigned long flags;
559   | 	u8 bir;
560   |
561   | 	pci_read_config_dword(dev, dev->msix_cap + PCI_MSIX_TABLE,
562   | 			      &table_offset);
563   | 	bir = (u8)(table_offset & PCI_MSIX_TABLE_BIR);
564   | 	flags = pci_resource_flags(dev, bir);
565   |  if (!flags || (flags & IORESOURCE_UNSET))
    26←Assuming 'flags' is not equal to 0→
    27←Assuming the condition is false→
    28←Taking false branch→
566   |  return NULL;
567   |
568   |  table_offset &= PCI_MSIX_TABLE_OFFSET;
569   |  phys_addr = pci_resource_start(dev, bir) + table_offset;
570   |
571   |  return ioremap(phys_addr, nr_entries * PCI_MSIX_ENTRY_SIZE);
    29←32-bit multiply widens to 64-bit after overflow; cast an operand to 64-bit before the multiply
572   | }
573   |
574   | /**
575   |  * msix_prepare_msi_desc - Prepare a half initialized MSI descriptor for operation
576   |  * @dev:	The PCI device for which the descriptor is prepared
577   |  * @desc:	The MSI descriptor for preparation
578   |  *
579   |  * This is separate from msix_setup_msi_descs() below to handle dynamic
580   |  * allocations for MSI-X after initial enablement.
581   |  *
582   |  * Ideally the whole MSI-X setup would work that way, but there is no way to
583   |  * support this for the legacy arch_setup_msi_irqs() mechanism and for the
584   |  * fake irq domains like the x86 XEN one. Sigh...
585   |  *
586   |  * The descriptor is zeroed and only @desc::msi_index and @desc::affinity
587   |  * are set. When called from msix_setup_msi_descs() then the is_virtual
588   |  * attribute is initialized as well.
589   |  *
590   |  * Fill in the rest.
591   |  */
592   | void msix_prepare_msi_desc(struct pci_dev *dev, struct msi_desc *desc)
593   | {
594   | 	desc->nvec_used				= 1;
595   | 	desc->pci.msi_attrib.is_msix		= 1;
596   | 	desc->pci.msi_attrib.is_64		= 1;
597   | 	desc->pci.msi_attrib.default_irq	= dev->irq;
598   | 	desc->pci.mask_base			= dev->msix_base;
599   | 	desc->pci.msi_attrib.can_mask		= !pci_msi_ignore_mask &&
600   | 						  !desc->pci.msi_attrib.is_virtual;
601   |
651   |
652   |  for (i = 0; i < tsize; i++, base += PCI_MSIX_ENTRY_SIZE)
653   |  writel(ctrl, base + PCI_MSIX_ENTRY_VECTOR_CTRL);
654   | }
655   |
656   | static int msix_setup_interrupts(struct pci_dev *dev, struct msix_entry *entries,
657   |  int nvec, struct irq_affinity *affd)
658   | {
659   |  struct irq_affinity_desc *masks = NULL;
660   |  int ret;
661   |
662   |  if (affd)
663   | 		masks = irq_create_affinity_masks(nvec, affd);
664   |
665   | 	msi_lock_descs(&dev->dev);
666   | 	ret = msix_setup_msi_descs(dev, entries, nvec, masks);
667   |  if (ret)
668   |  goto out_free;
669   |
670   | 	ret = pci_msi_setup_msi_irqs(dev, nvec, PCI_CAP_ID_MSIX);
671   |  if (ret)
672   |  goto out_free;
673   |
674   |  /* Check if all MSI entries honor device restrictions */
675   | 	ret = msi_verify_entries(dev);
676   |  if (ret)
677   |  goto out_free;
678   |
679   | 	msix_update_entries(dev, entries);
680   |  goto out_unlock;
681   |
682   | out_free:
683   | 	pci_free_msi_irqs(dev);
684   | out_unlock:
685   | 	msi_unlock_descs(&dev->dev);
686   | 	kfree(masks);
687   |  return ret;
688   | }
689   |
690   | /**
691   |  * msix_capability_init - configure device's MSI-X capability
692   |  * @dev: pointer to the pci_dev data structure of MSI-X device function
693   |  * @entries: pointer to an array of struct msix_entry entries
694   |  * @nvec: number of @entries
695   |  * @affd: Optional pointer to enable automatic affinity assignment
696   |  *
697   |  * Setup the MSI-X capability structure of device function with a
698   |  * single MSI-X IRQ. A return of zero indicates the successful setup of
699   |  * requested MSI-X entries with allocated IRQs or non-zero for otherwise.
700   |  **/
701   | static int msix_capability_init(struct pci_dev *dev, struct msix_entry *entries,
702   |  int nvec, struct irq_affinity *affd)
703   | {
704   |  int ret, tsize;
705   | 	u16 control;
706   |
707   |  /*
708   |  * Some devices require MSI-X to be enabled before the MSI-X
709   |  * registers can be accessed.  Mask all the vectors to prevent
710   |  * interrupts coming in before they're fully set up.
711   |  */
712   | 	pci_msix_clear_and_set_ctrl(dev, 0, PCI_MSIX_FLAGS_MASKALL |
713   |  PCI_MSIX_FLAGS_ENABLE);
714   |
715   |  /* Mark it enabled so setup functions can query it */
716   | 	dev->msix_enabled = 1;
717   |
718   | 	pci_read_config_word(dev, dev->msix_cap + PCI_MSIX_FLAGS, &control);
719   |  /* Request & Map MSI-X table region */
720   | 	tsize = msix_table_size(control);
721   |  dev->msix_base = msix_map_region(dev, tsize);
    25←Calling 'msix_map_region'→
722   |  if (!dev->msix_base) {
723   | 		ret = -ENOMEM;
724   |  goto out_disable;
725   | 	}
726   |
727   | 	ret = msix_setup_interrupts(dev, entries, nvec, affd);
728   |  if (ret)
729   |  goto out_disable;
730   |
731   |  /* Disable INTX */
732   | 	pci_intx_for_msi(dev, 0);
733   |
734   |  /*
735   |  * Ensure that all table entries are masked to prevent
736   |  * stale entries from firing in a crash kernel.
737   |  *
738   |  * Done late to deal with a broken Marvell NVME device
739   |  * which takes the MSI-X mask bits into account even
740   |  * when MSI-X is disabled, which prevents MSI delivery.
741   |  */
742   | 	msix_mask_all(dev->msix_base, tsize);
743   | 	pci_msix_clear_and_set_ctrl(dev, PCI_MSIX_FLAGS_MASKALL, 0);
744   |
745   | 	pcibios_free_irq(dev);
746   |  return 0;
747   |
748   | out_disable:
749   | 	dev->msix_enabled = 0;
750   | 	pci_msix_clear_and_set_ctrl(dev, PCI_MSIX_FLAGS_MASKALL | PCI_MSIX_FLAGS_ENABLE, 0);
751   |
752   |  return ret;
753   | }
754   |
755   | static bool pci_msix_validate_entries(struct pci_dev *dev, struct msix_entry *entries, int nvec)
756   | {
757   | 	bool nogap;
758   |  int i, j;
759   |
760   |  if (!entries)
761   |  return true;
762   |
763   | 	nogap = pci_msi_domain_supports(dev, MSI_FLAG_MSIX_CONTIGUOUS, DENY_LEGACY);
764   |
765   |  for (i = 0; i < nvec; i++) {
766   |  /* Check for duplicate entries */
767   |  for (j = i + 1; j < nvec; j++) {
768   |  if (entries[i].entry == entries[j].entry)
769   |  return false;
770   | 		}
771   |  /* Check for unsupported gaps */
772   |  if (nogap && entries[i].entry != i)
773   |  return false;
774   | 	}
775   |  return true;
776   | }
777   |
778   | int __pci_enable_msix_range(struct pci_dev *dev, struct msix_entry *entries, int minvec,
779   |  int maxvec, struct irq_affinity *affd, int flags)
780   | {
781   |  int hwsize, rc, nvec = maxvec;
782   |
783   |  if (maxvec < minvec)
    1Assuming 'maxvec' is >= 'minvec'→
    2←Taking false branch→
784   |  return -ERANGE;
785   |
786   |  if (dev->msi_enabled) {
    3←Assuming field 'msi_enabled' is 0→
    4←Taking false branch→
787   |  pci_info(dev, "can't enable MSI-X (MSI already enabled)\n");
788   |  return -EINVAL;
789   | 	}
790   |
791   |  if (WARN_ON_ONCE(dev->msix_enabled))
    5←Assuming field 'msix_enabled' is 0→
    6←Taking false branch→
    7←Taking false branch→
792   |  return -EINVAL;
793   |
794   |  /* Check MSI-X early on irq domain enabled architectures */
795   |  if (!pci_msi_domain_supports(dev, MSI_FLAG_PCI_MSIX, ALLOW_LEGACY))
    8←Assuming the condition is false→
796   |  return -ENOTSUPP;
797   |
798   |  if (!pci_msi_supported(dev, nvec) || dev->current_state != PCI_D0)
    9←Assuming the condition is false→
    10←Assuming field 'current_state' is equal to PCI_D0→
    11←Taking false branch→
799   |  return -EINVAL;
800   |
801   |  hwsize = pci_msix_vec_count(dev);
802   |  if (hwsize < 0)
    12←Assuming 'hwsize' is >= 0→
    13←Taking false branch→
803   |  return hwsize;
804   |
805   |  if (!pci_msix_validate_entries(dev, entries, nvec))
    14←Taking false branch→
806   |  return -EINVAL;
807   |
808   |  if (hwsize < nvec) {
    15←Assuming 'hwsize' is >= 'nvec'→
    16←Taking false branch→
809   |  /* Keep the IRQ virtual hackery working */
810   |  if (flags & PCI_IRQ_VIRTUAL)
811   | 			hwsize = nvec;
812   |  else
813   | 			nvec = hwsize;
814   | 	}
815   |
816   |  if (nvec16.1'nvec' is >= 'minvec' < minvec)
    17←Taking false branch→
817   |  return -ENOSPC;
818   |
819   |  rc = pci_setup_msi_context(dev);
820   |  if (rc17.1'rc' is 0)
    18←Taking false branch→
821   |  return rc;
822   |
823   |  if (!pci_setup_msix_device_domain(dev, hwsize))
    19←Assuming the condition is false→
    20←Taking false branch→
824   |  return -ENODEV;
825   |
826   |  for (;;) {
    21←Loop condition is true.  Entering loop body→
827   |  if (affd) {
    22←Assuming 'affd' is null→
    23←Taking false branch→
828   | 			nvec = irq_calc_affinity_vectors(minvec, nvec, affd);
829   |  if (nvec < minvec)
830   |  return -ENOSPC;
831   | 		}
832   |
833   |  rc = msix_capability_init(dev, entries, nvec, affd);
    24←Calling 'msix_capability_init'→
834   |  if (rc == 0)
835   |  return nvec;
836   |
837   |  if (rc < 0)
838   |  return rc;
839   |  if (rc < minvec)
840   |  return -ENOSPC;
841   |
842   | 		nvec = rc;
843   | 	}
844   | }
845   |
846   | void __pci_restore_msix_state(struct pci_dev *dev)
847   | {
848   |  struct msi_desc *entry;
849   | 	bool write_msg;
850   |
851   |  if (!dev->msix_enabled)
852   |  return;
853   |
854   |  /* route the table */
855   | 	pci_intx_for_msi(dev, 0);
856   | 	pci_msix_clear_and_set_ctrl(dev, 0,
857   |  PCI_MSIX_FLAGS_ENABLE | PCI_MSIX_FLAGS_MASKALL);
858   |
859   | 	write_msg = arch_restore_msi_irqs(dev);
860   |
861   | 	msi_lock_descs(&dev->dev);
862   |  msi_for_each_desc(entry, &dev->dev, MSI_DESC_ALL) {
863   |  if (write_msg)
